# =========================
# LLaMA Factory SFT config
# (STRICTLY aligned with your LLAF)
# =========================

### -------- Model --------
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
trust_remote_code: true

### -------- Stage --------
stage: sft
do_train: true
do_eval: false

### -------- Dataset --------
dataset: sft_train
dataset_dir: runs/latest

### -------- Length / Packing --------
cutoff_len: 4096
packing: false

### -------- Output --------
output_dir: outputs/qwen3-rft-sft
overwrite_output_dir: true

### -------- Training --------
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-5
num_train_epochs: 1.0

lr_scheduler_type: cosine
warmup_ratio: 0.03

bf16: true
fp16: false

logging_steps: 10
save_steps: 500
save_total_limit: 2

### -------- Optim --------
optim: adamw_torch
weight_decay: 0.0
max_grad_norm: 1.0

### -------- LoRA --------
finetuning_type: lora
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target: all

### -------- Dataloader --------
dataloader_num_workers: 2

### -------- Reproducibility --------
seed: 42
