{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RFT Codebase Functional Check Notebook\n",
        "This notebook validates the core *task → prompt → dataset preview → registry* components you uploaded.\n",
        "\n",
        "It is designed to run **even before** the full `RFT/src/rft/...` package layout is in place, by loading modules directly from file paths.\n",
        "\n",
        "## Files under test\n",
        "- `pyproject.toml`\n",
        "- `spec.py`, `registry.py`, `io.py`\n",
        "- `templates.py`, `render.py`\n",
        "\n",
        "## What this notebook checks\n",
        "1. Packaging metadata sanity (Python version constraints, scripts)\n",
        "2. `TaskSpec` validation logic\n",
        "3. `TaskRegistry` loads CSV/JSONL and constructs `TaskSpec`\n",
        "4. Dataset preview token wrapping and model-visible input\n",
        "5. Prompt rendering into ShareGPT-style messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19508a0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"CWD =\", Path(\".\").resolve())\n",
        "print(\"Files here =\", list(Path(\".\").iterdir()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Paths to uploaded files (adjust if you moved them) ---\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\".\").resolve()\n",
        "\n",
        "PYPROJECT = ROOT / \"pyproject.toml\"\n",
        "SPEC_PY   = ROOT / \"spec.py\"\n",
        "REG_PY    = ROOT / \"registry.py\"\n",
        "IO_PY     = ROOT / \"io.py\"\n",
        "TPL_PY    = ROOT / \"templates.py\"\n",
        "RENDER_PY = ROOT / \"render.py\"\n",
        "\n",
        "for p in [PYPROJECT, SPEC_PY, REG_PY, IO_PY, TPL_PY, RENDER_PY]:\n",
        "    print(p, \"exists =\", p.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Utility: import a module from a file path ---\n",
        "import importlib.util\n",
        "from types import ModuleType\n",
        "\n",
        "def load_module(name: str, path: Path) -> ModuleType:\n",
        "    spec = importlib.util.spec_from_file_location(name, str(path))\n",
        "    if spec is None or spec.loader is None:\n",
        "        raise ImportError(f\"Cannot load module {name} from {path}\")\n",
        "    mod = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(mod)  # type: ignore[attr-defined]\n",
        "    return mod\n",
        "\n",
        "spec_mod   = load_module(\"rft_tasks_spec\", SPEC_PY)\n",
        "registry_mod = load_module(\"rft_tasks_registry\", REG_PY)\n",
        "io_mod     = load_module(\"rft_tasks_io\", IO_PY)\n",
        "tpl_mod    = load_module(\"rft_prompt_templates\", TPL_PY)\n",
        "render_mod = load_module(\"rft_prompt_render\", RENDER_PY)\n",
        "\n",
        "print(\"Loaded:\", spec_mod, registry_mod, io_mod, tpl_mod, render_mod)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- 1) Packaging sanity checks ---\n",
        "import tomllib\n",
        "\n",
        "data = tomllib.loads(PYPROJECT.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "print(\"project.name:\", data[\"project\"][\"name\"])\n",
        "print(\"project.requires-python:\", data[\"project\"].get(\"requires-python\"))\n",
        "print(\"project.scripts:\", list(data.get(\"project\", {}).get(\"scripts\", {}).keys()))\n",
        "\n",
        "# Check that the core CLI entrypoints exist (they may not yet, but we flag it)\n",
        "expected_scripts = {\"rft-generate\",\"rft-verify\",\"rft-build\",\"rft-train\",\"rft-eval\"}\n",
        "missing = expected_scripts - set(data.get(\"project\", {}).get(\"scripts\", {}).keys())\n",
        "print(\"Missing scripts:\", missing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- 2) TaskSpec unit checks ---\n",
        "TaskSpec = spec_mod.TaskSpec\n",
        "\n",
        "# A valid minimal TaskSpec should validate\n",
        "t = TaskSpec(task_id=\"demo_task\", split=\"sab\", eval_entrypoint=\"python -m benchmark.eval_programs.eval_demo\")\n",
        "t.validate()\n",
        "print(\"TaskSpec.validate OK:\", t.short_name())\n",
        "\n",
        "# Missing eval_entrypoint must fail\n",
        "try:\n",
        "    TaskSpec(task_id=\"bad\", split=\"sab\", eval_entrypoint=\"\").validate()\n",
        "    raise AssertionError(\"Expected validation failure but got success\")\n",
        "except ValueError as e:\n",
        "    print(\"Expected failure:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- 3) Create a tiny annotation table and test TaskRegistry ---\n",
        "import tempfile, csv, json\n",
        "from pathlib import Path\n",
        "\n",
        "TaskRegistry = registry_mod.TaskRegistry\n",
        "\n",
        "with tempfile.TemporaryDirectory() as td:\n",
        "    td = Path(td)\n",
        "    ann = td / \"ann.csv\"\n",
        "    with ann.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=[\"task_id\",\"task_inst\",\"eval_script_name\",\"dataset_preview\",\"domain\",\"subtask_categories\"])\n",
        "        w.writeheader()\n",
        "        w.writerow({\n",
        "            \"task_id\": \"toy_1\",\n",
        "            \"task_inst\": \"Write a program that prints 'hello'.\",\n",
        "            \"eval_script_name\": \"eval_toy_1.py\",\n",
        "            \"dataset_preview\": \"a,b\\n1,2\\n\",\n",
        "            \"domain\": \"toy\",\n",
        "            \"subtask_categories\": \"io,printing\"\n",
        "        })\n",
        "\n",
        "    reg = TaskRegistry(annotation_path=ann, benchmark_root=Path(\".\"), split=\"sab\")\n",
        "    tasks = reg.list_tasks()\n",
        "    print(\"Num tasks:\", len(tasks))\n",
        "    print(\"First task:\", tasks[0])\n",
        "\n",
        "    ts = reg.get(\"toy_1\")\n",
        "    print(\"eval_entrypoint:\", ts.eval_entrypoint)\n",
        "    assert ts.eval_entrypoint.endswith(\"benchmark.eval_programs.eval_toy_1\"), \"entrypoint resolution mismatch\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- 4) Dataset preview tokens + model-visible input ---\n",
        "PREVIEW_START_TOKEN = io_mod.PREVIEW_START_TOKEN\n",
        "PREVIEW_END_TOKEN   = io_mod.PREVIEW_END_TOKEN\n",
        "\n",
        "ts = TaskSpec(\n",
        "    task_id=\"toy_2\",\n",
        "    split=\"sab\",\n",
        "    instruction=\"Do something.\",\n",
        "    dataset_preview=\"x,y\\n3,4\\n\",\n",
        "    eval_entrypoint=\"python -m benchmark.eval_programs.eval_toy_2\"\n",
        ")\n",
        "\n",
        "preview = io_mod.get_dataset_preview(ts)\n",
        "print(preview)\n",
        "assert preview.startswith(PREVIEW_START_TOKEN)\n",
        "assert preview.strip().endswith(PREVIEW_END_TOKEN)\n",
        "\n",
        "visible = io_mod.get_model_visible_input(ts)\n",
        "print(\"\\n--- model visible ---\\n\", visible)\n",
        "assert \"Do something.\" in visible\n",
        "assert PREVIEW_START_TOKEN in visible\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- 5) Prompt template and render() contract checks ---\n",
        "template = tpl_mod.THINKING_CODE_TEMPLATE\n",
        "assert \"<thinking>\" in template and \"</thinking>\" in template, \"Template missing thinking tags\"\n",
        "assert \"```python\" in template, \"Template missing python fenced block instruction\"\n",
        "\n",
        "msgs = render_mod.render(ts)\n",
        "print(msgs)\n",
        "assert isinstance(msgs, list) and len(msgs) == 2\n",
        "assert msgs[0][\"role\"] == \"system\"\n",
        "assert msgs[1][\"role\"] == \"user\"\n",
        "assert msgs[0][\"content\"] == template\n",
        "assert \"Do something.\" in msgs[1][\"content\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "If all cells above run successfully, these components are functionally consistent.\n",
        "\n",
        "To validate the full pipeline later, you will extend this notebook to:\n",
        "1. Call a vLLM endpoint and write `candidates.jsonl`\n",
        "2. Parse model output into `thinking` + `program`\n",
        "3. Run your verifier entrypoints in a sandbox and emit `verdicts.jsonl`\n",
        "4. Build SFT/DPO training JSONL files for LLaMA Factory\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rft",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
