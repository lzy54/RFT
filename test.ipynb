{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "112519b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lizhuoyang/Desktop/RFT/pyproject.toml exists = True\n",
      "/Users/lizhuoyang/Desktop/RFT/src/rft/tasks/spec.py exists = True\n",
      "/Users/lizhuoyang/Desktop/RFT/src/rft/tasks/registry.py exists = True\n",
      "/Users/lizhuoyang/Desktop/RFT/src/rft/tasks/io.py exists = True\n",
      "/Users/lizhuoyang/Desktop/RFT/src/rft/prompt/templates.py exists = True\n",
      "/Users/lizhuoyang/Desktop/RFT/src/rft/prompt/render.py exists = True\n"
     ]
    }
   ],
   "source": [
    "# --- Paths to uploaded files (adjust if you moved them) ---\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "\n",
    "PYPROJECT = ROOT / \"pyproject.toml\"\n",
    "SPEC_PY   = ROOT / \"src\" / \"rft\" / \"tasks\" / \"spec.py\"\n",
    "REG_PY    = ROOT / \"src\" / \"rft\" / \"tasks\" / \"registry.py\"\n",
    "IO_PY     = ROOT / \"src\" / \"rft\" / \"tasks\" / \"io.py\"\n",
    "TPL_PY    = ROOT / \"src\" / \"rft\" / \"prompt\" / \"templates.py\"\n",
    "RENDER_PY = ROOT / \"src\" / \"rft\" / \"prompt\" / \"render.py\"\n",
    "\n",
    "for p in [PYPROJECT, SPEC_PY, REG_PY, IO_PY, TPL_PY, RENDER_PY]:\n",
    "    print(p, \"exists =\", p.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88fe217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: <module 'rft_tasks_spec' from '/Users/lizhuoyang/Desktop/RFT/src/rft/tasks/spec.py'> <module 'rft_tasks_registry' from '/Users/lizhuoyang/Desktop/RFT/src/rft/tasks/registry.py'> <module 'rft_tasks_io' from '/Users/lizhuoyang/Desktop/RFT/src/rft/tasks/io.py'> <module 'rft_prompt_templates' from '/Users/lizhuoyang/Desktop/RFT/src/rft/prompt/templates.py'> <module 'rft_prompt_render' from '/Users/lizhuoyang/Desktop/RFT/src/rft/prompt/render.py'>\n"
     ]
    }
   ],
   "source": [
    "# --- Utility: import a module from a file path (SAFE VERSION) ---\n",
    "import importlib.util\n",
    "import sys\n",
    "from types import ModuleType\n",
    "from pathlib import Path\n",
    "\n",
    "def load_module(name: str, path: Path) -> ModuleType:\n",
    "    spec = importlib.util.spec_from_file_location(name, str(path))\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise ImportError(f\"Cannot load module {name} from {path}\")\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "\n",
    "    # ⭐ 关键修复：注册到 sys.modules\n",
    "    sys.modules[name] = mod\n",
    "\n",
    "    spec.loader.exec_module(mod)  # type: ignore[attr-defined]\n",
    "    return mod\n",
    "\n",
    "\n",
    "spec_mod   = load_module(\"rft_tasks_spec\", SPEC_PY)\n",
    "registry_mod = load_module(\"rft_tasks_registry\", REG_PY)\n",
    "io_mod     = load_module(\"rft_tasks_io\", IO_PY)\n",
    "tpl_mod    = load_module(\"rft_prompt_templates\", TPL_PY)\n",
    "render_mod = load_module(\"rft_prompt_render\", RENDER_PY)\n",
    "\n",
    "print(\"Loaded:\", spec_mod, registry_mod, io_mod, tpl_mod, render_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5486269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project.name: rft\n",
      "project.requires-python: >=3.10,<3.11\n",
      "project.scripts: ['rft-generate', 'rft-verify', 'rft-build', 'rft-train', 'rft-eval']\n",
      "Missing scripts: set()\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Packaging sanity checks ---\n",
    "try:\n",
    "    import tomllib  # Python >= 3.11\n",
    "except ModuleNotFoundError:\n",
    "    import tomli as tomllib  # Python <= 3.10\n",
    "\n",
    "data = tomllib.loads(PYPROJECT.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"project.name:\", data[\"project\"][\"name\"])\n",
    "print(\"project.requires-python:\", data[\"project\"].get(\"requires-python\"))\n",
    "print(\"project.scripts:\", list(data.get(\"project\", {}).get(\"scripts\", {}).keys()))\n",
    "\n",
    "expected_scripts = {\"rft-generate\", \"rft-verify\", \"rft-build\", \"rft-train\", \"rft-eval\"}\n",
    "missing = expected_scripts - set(data.get(\"project\", {}).get(\"scripts\", {}).keys())\n",
    "print(\"Missing scripts:\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5ebd0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskSpec.validate OK: sab:demo_task\n",
      "Expected failure: TaskSpec.eval_entrypoint missing for task 'bad'\n"
     ]
    }
   ],
   "source": [
    "# --- 2) TaskSpec unit checks ---\n",
    "TaskSpec = spec_mod.TaskSpec\n",
    "\n",
    "# A valid minimal TaskSpec should validate\n",
    "t = TaskSpec(task_id=\"demo_task\", split=\"sab\", eval_entrypoint=\"python -m benchmark.eval_programs.eval_demo\")\n",
    "t.validate()\n",
    "print(\"TaskSpec.validate OK:\", t.short_name())\n",
    "\n",
    "# Missing eval_entrypoint must fail\n",
    "try:\n",
    "    TaskSpec(task_id=\"bad\", split=\"sab\", eval_entrypoint=\"\").validate()\n",
    "    raise AssertionError(\"Expected validation failure but got success\")\n",
    "except ValueError as e:\n",
    "    print(\"Expected failure:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3106260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tasks: 1\n",
      "First task: TaskSpec(task_id='toy_1', split='sab', version='v1', domain='toy', subtask_categories=['io', 'printing'], instruction=\"Write a program that prints 'hello'.\", dataset_preview='a,b\\n1,2\\n', input_format_hint=None, eval_entrypoint='python -m benchmark.eval_programs.eval_toy_1', eval_timeout_sec=1800, max_memory_mb=4096, execution_env={}, evaluation_params={}, sampling_hint={}, metadata={})\n",
      "eval_entrypoint: python -m benchmark.eval_programs.eval_toy_1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3) Create a tiny annotation table and test TaskRegistry ---\n",
    "import tempfile, csv, json\n",
    "from pathlib import Path\n",
    "\n",
    "TaskRegistry = registry_mod.TaskRegistry\n",
    "\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    td = Path(td)\n",
    "    ann = td / \"ann.csv\"\n",
    "    with ann.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"task_id\",\"task_inst\",\"eval_script_name\",\"dataset_preview\",\"domain\",\"subtask_categories\"])\n",
    "        w.writeheader()\n",
    "        w.writerow({\n",
    "            \"task_id\": \"toy_1\",\n",
    "            \"task_inst\": \"Write a program that prints 'hello'.\",\n",
    "            \"eval_script_name\": \"eval_toy_1.py\",\n",
    "            \"dataset_preview\": \"a,b\\n1,2\\n\",\n",
    "            \"domain\": \"toy\",\n",
    "            \"subtask_categories\": \"io,printing\"\n",
    "        })\n",
    "\n",
    "    reg = TaskRegistry(annotation_path=ann, benchmark_root=Path(\".\"), split=\"sab\")\n",
    "    tasks = reg.list_tasks()\n",
    "    print(\"Num tasks:\", len(tasks))\n",
    "    print(\"First task:\", tasks[0])\n",
    "\n",
    "    ts = reg.get(\"toy_1\")\n",
    "    print(\"eval_entrypoint:\", ts.eval_entrypoint)\n",
    "    assert ts.eval_entrypoint.endswith(\"benchmark.eval_programs.eval_toy_1\"), \"entrypoint resolution mismatch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "915163a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START Preview of Dataset]\n",
      "x,y\n",
      "3,4\n",
      "[END Preview of Dataset]\n",
      "\n",
      "--- model visible ---\n",
      " Do something.\n",
      "\n",
      "[START Preview of Dataset]\n",
      "x,y\n",
      "3,4\n",
      "[END Preview of Dataset]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4) Dataset preview tokens + model-visible input ---\n",
    "PREVIEW_START_TOKEN = io_mod.PREVIEW_START_TOKEN\n",
    "PREVIEW_END_TOKEN   = io_mod.PREVIEW_END_TOKEN\n",
    "\n",
    "ts = TaskSpec(\n",
    "    task_id=\"toy_2\",\n",
    "    split=\"sab\",\n",
    "    instruction=\"Do something.\",\n",
    "    dataset_preview=\"x,y\\n3,4\\n\",\n",
    "    eval_entrypoint=\"python -m benchmark.eval_programs.eval_toy_2\"\n",
    ")\n",
    "\n",
    "preview = io_mod.get_dataset_preview(ts)\n",
    "print(preview)\n",
    "assert preview.startswith(PREVIEW_START_TOKEN)\n",
    "assert preview.strip().endswith(PREVIEW_END_TOKEN)\n",
    "\n",
    "visible = io_mod.get_model_visible_input(ts)\n",
    "print(\"\\n--- model visible ---\\n\", visible)\n",
    "assert \"Do something.\" in visible\n",
    "assert PREVIEW_START_TOKEN in visible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbb76cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a careful, rigorous scientific coding agent.\\n\\nYou will be given:\\n- A TASK INSTRUCTION\\n- Optionally, a DATASET PREVIEW\\n\\nYour job is to reason step by step and then write a complete,\\nexecutable Python program that satisfies the task.\\n\\nYour output will be evaluated by an automatic evaluation script.\\nOnly correct and executable programs will be accepted.\\n\\n============================================================\\nOUTPUT FORMAT (STRICT — MUST FOLLOW EXACTLY)\\n============================================================\\n\\n1. Output exactly ONE <thinking>...</thinking> block.\\n2. Then output exactly ONE Python code block fenced by ```python and ``` .\\n3. Do NOT output any extra text before, between, or after these blocks.\\n4. Do NOT include Markdown fences inside the Python code block.\\n\\n============================================================\\n<thinking>\\n============================================================\\n\\nIn this block:\\n- Write your step-by-step reasoning and plan.\\n- Clearly state assumptions.\\n- Describe the algorithm and key steps.\\n- Mention edge cases and sanity checks.\\n- Do NOT include any code here.\\n\\n============================================================\\nPython code block\\n============================================================\\n\\nIn the Python code block:\\n- Write a complete, self-contained, executable Python program.\\n- Follow the task instruction precisely.\\n- Use only Python standard libraries and libraries implied by the task context.\\n- Assume the current working directory is writable.\\n- Write outputs to the expected files or locations required by the task.\\n- Handle errors explicitly with clear exceptions or messages.\\n- The program MUST run when executed as a script.\\n\\n============================================================\\nBEGIN OUTPUT\\n============================================================\\n\\n<thinking>\\n</thinking>\\n\\n```python\\ndef main():\\n    pass\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n\\n'}, {'role': 'user', 'content': 'Do something.\\n\\n[START Preview of Dataset]\\nx,y\\n3,4\\n[END Preview of Dataset]'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5) Prompt template and render() contract checks ---\n",
    "template = tpl_mod.THINKING_CODE_TEMPLATE\n",
    "assert \"<thinking>\" in template and \"</thinking>\" in template, \"Template missing thinking tags\"\n",
    "assert \"```python\" in template, \"Template missing python fenced block instruction\"\n",
    "\n",
    "msgs = render_mod.render(ts)\n",
    "print(msgs)\n",
    "assert isinstance(msgs, list) and len(msgs) == 2\n",
    "assert msgs[0][\"role\"] == \"system\"\n",
    "assert msgs[1][\"role\"] == \"user\"\n",
    "assert msgs[0][\"content\"] == template\n",
    "assert \"Do something.\" in msgs[1][\"content\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
